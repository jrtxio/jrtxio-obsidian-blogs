---
{"dg-publish":true,"dg-path":"编程语言/人算不如天算：之电脑中的微型大脑.md","permalink":"/编程语言/人算不如天算：之电脑中的微型大脑/","created":"2023-12-14T14:36:49.000+08:00","updated":"2024-12-31T10:04:30.000+08:00"}
---

#Technomous #PLT

![Pasted image 20231214143706.png|250](/img/user/0.Asset/resource/Pasted%20image%2020231214143706.png)

你的大脑有多强大，你知道吗？

![Pasted image 20231214143825.gif|250](/img/user/0.Asset/resource/Pasted%20image%2020231214143825.gif)

看看这幅图，它是由两幅图像交替而制成的gif动画，是不是很有立体感？不需要多少思考，大多数人一瞄就能说出壁炉上各种器皿的位置或者木柴的大概纹理（伴随着晕眩的副作用）。但如果要通过计算机来建立立体模型的话，要先通过轮廓提取之类的方法找出两幅图像之间的对应点，然后计算相机的位置，最后才能用几何的方法将立体模型建造出来，这个过程涉及不少复杂的矩阵计算。但人脑似乎用某种奇异的方法，不费吹灰之力就完成了同样的任务。

不仅是人脑，似乎简单得多的动物，它们的脑在某些方面也有出众的表现。澳大利亚的一组研究人员就花了相当时间对苍蝇脑的视觉部分进行了分析，将其中一些神经元的活动以方程的形式提炼出来，结果得到了一个图像运动检测算法，在很多情况下比目前的算法效果要更好，效率也更高，但算法的整体原理在很大程度上仍然是个谜。

那为什么脑，或者说由神经元构成的网络，会有这么神秘而强大的能力呢？

研究大脑的学者恐怕会说：这个嘛，不太清楚。

对于大脑产生智能的具体机理，学界有很多不同的观点，但是在目前的研究水平上，不要说大脑的整体运作，就算是大脑的神经元之间的互动，也还藏有不少的谜团，所以，我们离揭示大脑能力的奥秘还差很远很远。要更进一步，还需要更精细的分析能力和更强大的计算能力。

但工程师有工程师的想法。他们相对而言不太关心具体的机理，更关注的是这东西是否管用。比如说从苍蝇脑里提取的算法，尽管研究人员既不明白每一步的意义，也不明白这个算法是如何一步一步演化形成的，但这不妨碍他们对这个神秘的算法的应用。据报道，已经有个别自动导航系统在使用这个算法了。

同样，对于大脑，工程师们想到的是：如果对神经元组成的网络进行适当的模拟，或许可以完成对于传统算法来说困难的任务。沿着这个思路，他们得到了“人工神经网络”，一类被广泛应用的人工智能算法。

# 简单的拼板

要模拟生物中的神经网络，那就要明白神经细胞之间通讯的大概原理。

![Pasted image 20231214143826.png|250](/img/user/0.Asset/resource/Pasted%20image%2020231214143826.png)

神经元细胞的构造相当复杂，主要由细胞体、树突和轴突构成。细胞体负责各种后勤和调控工作，而树突和轴突的功能则是完成细胞之间的通讯。树突负责接收其它细胞的信号，而轴突负责把细胞的信号传给别的细胞。如果神经细胞接收到的信息满足一定的条件，它就会被激活，然后开始向其它细胞传递信号。具体的信号处理与传递过程相当复杂，甚至还涉及其他种类的细胞。

但是，对于工程师来说，这样的描述甚至仍然太复杂了。本着抽象和过度简化的原则，他们建立了神经细胞的一个非常简化的模型：人工神经元。

![Pasted image 20231214143839.png|250](/img/user/0.Asset/resource/Pasted%20image%2020231214143839.png)

一个人工神经元可以分为三个部分。第一部分是输入，负责接收来自外部的信号；第二部分是信息处理，它将来自其它神经元的信号，按照信号的性质和重要性进行汇总后，再根据一定的条件判断是否发出信号，以及发出信号的强度；最后一部分是输出，将信号输出到外部，有可能是到别的神经元，也有可能直接作为神经网络的输出。

这是个相当简单的模型，真实的神经细胞运作的方式远远要更复杂。简单的模型好处在于编程和分析的难度都比较低。从实用性的角度来看，这要比实打实地模拟一个神经细胞更划算。

# 容易的拼图

接下来的工作，就是要将人工神经元拼起来，做成一个人工神经网络。这一步看起来不难，但到底怎么拼，要拼成什么样的结构，这仍然是个问题。

一个看起来明智的选择就是直接模拟大脑结构，也就是采用带有反馈的结构。这样一来更贴近大脑，二来也给充分利用了人工神经元之间的连接，有着更大的复杂性，看起来也更强大。

这是一个典型的“科学家的选择“，工程师们对此就可能不太同意。这种带有反馈的结构的确可能有更强大的性能，但它的复杂性也增加了编程和分析的难度。在“性价比”的考量下，对工程师而言，以下的结构（被称为“前馈结构”）可能更适合。

![Pasted image 20231214143906.png|250](/img/user/0.Asset/resource/Pasted%20image%2020231214143906.png)

这样的结构，是由几层人工神经元搭起来的。第一层神经元接受输入，最后一层神经元输出结果，而中间的每一层神经元，只能接受上面一层神经元的信号，然后将处理过的信号输出到下一层的神经元。因为这样的层状结构中不会存在信号的反馈，所以这种结构就被称为多层前馈结构了。

选择这种前馈结构的原因，自然是因为结构简单，但另一方面，它的性能是否足够强大呢？答案是肯定的。从数学上可以证明，在适当的条件下，这种前馈结构的人工神经网络，只要规模足够大，就可以对任意的连续函数进行足够好的近似。这个能力就足以（在近似的意义上）解决现实中的绝大部分问题了。

当然，也有研究人员在对其它结构进行研究，也得到了一些不错的结果。但在目前来看，简单有效的前馈结构在应用中仍然是非常热门的。

# 牵引的连接

即使选择好了结构，工作还远远没有完成。我们仍然需要确定对于整个网络至关重要的一些参数。方才提到，人工神经元会对外部传来的信号按照性质和重要性进行汇总，然后决定发出多强烈的信号。在这里，如何进行信号的汇总，以及按照什么方法决定发出信号的强烈程度，都是需要确定的。

我们当然希望模拟真实的神经细胞，毕竟它是我们唯一能模仿的对象。然而，真实的神经细胞的有关机制实在过于复杂了，我们目前仍不能窥其全貌，而且在工程学上来说，完整模拟这样复杂的机制也不划算。于是，我们只能退而求其次，尝试用尽量简单的机制来完成任务。

面对汇总的问题，一个经常用到的解决办法就是加权求和。对于每一个神经元，我们可以对每个输入赋予一个权重，汇总时，将每个输入的信号强度乘以相应的权重，然后求和。最后得到的结果，可以看成是输入的汇总。

接下来是决定发出多强烈的信号。对神经细胞发出信号的具体机制，我们仍不十分了解，但我们总可以假定，神经细胞发出的信号是关于接收到的信号的一个函数。类比到人工神经元的话，我们可以认为，人工神经元发出的信号就是关于输入汇总的一个函数。在实际应用中，一类经常被用到的函数是sigmoid函数。这类函数便于计算和性能分析，而又有真实神经元非线性的性质。

那么，为什么我们不选择更简单的线性函数y=ax+b呢？一个原因是，如果我们这样做的话，由于系统的其它部分都是线性的，整个系统可以轻易简化为一个线性函数。这样的话，我们就丢失了好不容易通过结构和连接创造出来的复杂性了，而这样简单的系统也不能满足我们解决复杂问题的需要。

现在，万事俱备只欠东风。整个人工神经网络中，只剩下一类参数还没有确定，那就是神经元之间连接的权重，但这恰好也是最难解决的问题。

我们在前面曾经提到一个定理：对于任意一个函数，只要我们有一个足够大的前馈结构的神经网络，我们都可以以任意精度来近似这个函数。这种对于任意函数的适应性，正是来源于我们可以自由选取人工神经元之间连接权重的这个能力。也就是说，对于不同的问题，我们需要选择不同的权重组合，而这个权重组合的计算方法，并不是显而易见的。

这个问题，对于自然中的神经网络——也就是各种生物的脑——也是存在的。那么，自然是怎么解决这个问题的吗？

答案就是：通过学习。

# 点睛的一笔

我们的人工神经网络，在很多方面都跟一只刚出生的小狗很相似：虽然蕴含着巨大的潜力，但一开始什么也不能做。既然我们可以训练小狗完成各种任务，那么，我们能不能想到一个办法，通过“训练”来确定人工神经网络的权重，从而“教会”它那些我们需要它完成的任务呢？

回想一下我们是怎么训练小狗“坐下”的。一开始我们下“坐下”的命令的时候，小狗通常会不知所措，因为它也不明白人类的语言。但偶尔它也会坐下来，这时候我们就会给它点好吃的或者摸摸头作为奖励。久而久之，这种服从命令的行为就会被逐渐强化，最后，小狗听到“坐下”的命令就会乖乖地坐下来。

对人工神经网络的训练，与训练小狗的方法其实相差不远：让它面对各种可能的情况，然后根据它做出的判断正确与否，适当地调整人工神经元之间的权重。比如说，在判断正确时，我们可以适当强化相关的人工神经元之间的连接，而在判断错误时则弱化之。在经过大量的重复训练之后，我们可以期望得到的人工神经网络能足够好地完成我们给与的任务。

但这也引入了另一个问题：如何根据人工神经网络输出的结果去调整权重？对于一般结构的神经网络来说，方法并不是显而易见的。幸好，对于我们考虑的多层前馈结构来说，我们有一个相对简单的权重调节方法，叫反向传递法（back-propagation）。这种方法的实质就是找到对判断有影响的连接，然后适当修改它们的权值。这也是我们在应用中更青睐多层前馈结构的一个原因。

到这里，大部分技术上的问题都已经解决了。对于我们选取的多层前馈网络，要训练它完成某项任务，我们先收集各种可能的情况以及应该作出的判断，然后根据这些数据利用反向传递法对人工神经网络进行大量的训练，直到它的表现足够好为止。这样得到的人工神经网络，在理论上应该就能足够好地完成任务了。

在这个过程中，仍有一些细节是需要注意的。

训练开始时，我们先给人工神经网络赋予随机的权重，避免手动选择权重所可能导致的偏差。这样的话，最后得到的人工神经网络的表现就仅仅依赖于我们用于训练的数据了。

如果训练使用的数据不够全面的话，训练出来的人工神经网络的可靠性也会降低。试想一下，如果一位厨师学徒，每天不停在练的都是粤菜，某天忽然让他做个酱骨架，那可能就做得淡而无味了。人工神经网络也是一样，要想它更好地应对各种情况的话，训练时就要让它“见识”更多不同的数据。

上面说到的学习方法，术语叫有监督学习，顾名思义就是在已经知道正确答案的情况下进行训练。实际上对于人工神经网络来说，别的学习模式（如无监督学习、强化学习）也是可以应用的，不同的学习方法可以解决不同领域的任务。

最后一点，人工神经网络并不是万能的。跟我们一样，它有时也会犯错。用人工神经网络解决某个问题，实际上就是将这个问题表达成某种统计的形式来简化解答，而在这个过程中不免会丢失原来问题的信息，使得解答并非总是准确。不过想到作为万物之灵的人类也经常犯错，人工神经网络的这点问题还是可以理解的。

# 美妙的画卷

但工程师并不满足于理论上的结果，他们是实践主义者，仅仅是理论上的可行性并不能成为他们接受一项新技术的理由。打个比方，理论上来说，每条河都有个源头，而要找到这个源头，只要溯河而上就可以了。殊不知当年为了探寻尼罗河的源头，有多少勇者败给了沿途的险恶环境而客死异乡。理论可以指出一条路，但这条路具体是好是坏，还是要脚踏实地走一走才知道。

幸而人工神经网络不负众望，在许多领域都有不俗的表现，如医学诊断、图像识别等。人工神经网络的灵活性，是传统的计算机程序所难以比拟的。在一些需要一定灵活性的领域中，当传统的计算机程序难以解决问题时，人工神经网络就能派上用场了。

然而，相比起自然中的神经网络，人工神经网络仍然不足挂齿。无论是在结构还是在运作方式上，人工神经网络都只是动物神经系统的一个极端简化的版本，在适应性和复杂度上远远逊色于原版。但最重要的区别是规模，大自然可以将数百亿计的神经元轻易安排在我们头骨以下那几立方分米的空间中，而我们想要模拟这些神经元，就必须动用最强的超级计算机，而且还需要很长的时间才能得到一些模糊的结果。

另一个重要的区别在于学习。我们的大脑可以轻易学习各种复杂的技能，还能根据以往的经验举一反三，但对于如此高效的学习过程，科学家仍然不清楚它的具体机理，甚至不清楚它是怎么发生的。可以想象，假以时日，当科学家们看清楚大脑的各种机理之后，会给工程师们指引一条改进人工神经网络的道路。

但道路的尽头，仍然是手执数十亿年时间优势的自然。在如此漫长的时光中，即使只有试错法，自然的创造仍然是无可比拟地美丽和强大。作为造物的我们，对自然各种创造的伟大和美妙之处的理解，才刚刚开始。