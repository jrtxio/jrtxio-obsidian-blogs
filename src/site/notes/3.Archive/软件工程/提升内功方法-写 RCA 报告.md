---
{"dg-publish":true,"dg-path":"软件工程/提升内功方法-写 RCA 报告.md","permalink":"/软件工程/提升内功方法-写 RCA 报告/","created":"2025-06-16T14:14:56.000+08:00","updated":"2025-06-16T14:16:29.000+08:00"}
---

#Innolight

RCA（Root Cause Analysis，根本原因分析）是一种用于分析失败的程序、技术问题、意外事件等所导致任务未完成根本原因的方法。目前一些工业型企业在使用该方法（如果你去网上找资料，大多都是工业型企业的培训 PPT）。RCA 其实很简单，它存在的目的就是为了找出问题发生的根本原因，然后找出解决问题的方法和制定预防措施。

这种严谨的方法同样也适用到我们的技术领域，在实践过程中对遇到的问题或者 bug 刨根问底，然后总结下来，如此长期积累，不仅可以锻炼处理问题的能力，也可以提高自己的技术内功。很多技术员在遇到问题仅仅是停留在按网上说的“删除某个文件”、“改下这个配置”就完事的表面形式，缺乏去思考 why。

对于解决的问题，我们连续问自己 5 个 why（有时可能会问到更多或更少），直到回答出根本原因，然后把它表达出来。我看了一些传统企业讲 RCA 的 PPT，讲了很多 RCA 表示方法，比如鱼骨图、关联图等，但对我们技术员来说最简单的方法就是记录成文字即可，只需包含关键要素：

1、定义问题，表面原因是 xxxx；
2、分析问题，根本原因是 xxxx；
3、总结解决方案，形成预防措施。

下面是我常用的模板：

```
1、问题现象
所产生问题有着什么表现，比如内存不足、CPU 占用率过高等。

2、分析原因
用前面提到的 5 个 why，不断问前一个问题为什么发生，直到分析到根本原因。所谓根本原因，就是精准分析出问题所在点，找不到更多的理由含糊过去。

3、解决方案
出现这种现象，应该如何去解决。

4、经验总结
将来可以采取什么措施来避免这类问题的发生。
```

我举个例子，某次解决 Spark 小文件问题记录的 RCA：

```
问题现象：
导出了 12 个月的日志，每个月日志大概 100M，导出的是 Parquet 格式。但是用 spark.read.parquet 载入新导出的日志时，SparkShell 会长时间阻塞，如果中断进程时会发现 GC 异常，有时运行时间长了也会报 GC 问题。

问题原因分析：
统计导出来的日志（Parquet 格式，后缀名 .parquet），共有 1,655,329个，用 Spark 一次性读这么多文件时，由于 Spark 会把文件等信息都载入内存，竟而增加了 GC 的工作压力，所以 JVM 崩溃。

产生那么多小文件的原因是每个 task 都会生成一个 .parquet 文件。

解决方法：
只用对小文件做合并操作即可：

spark.sql("select * from logs").coalesce(1024).write.parquet(path)

coalesce 参数表示生成的文件最多 1024 个。

经验教训：
如果是导长时间的数据，write 操作前就应该加上 coalesce。
```